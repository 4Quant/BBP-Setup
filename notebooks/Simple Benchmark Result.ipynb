{"cells":[{"cell_type":"code","source":["import pyspark\n# sc = pyspark.SparkContext(pyspark.SparkConf().setMaster(\"local[12]\"))\nimport numpy as np\nimport time\nfrom collections import namedtuple\ntile_id = namedtuple('tile_id', ['img_id', 'z', 'x', 'y', 'type'])\ntile_item = namedtuple('tile_item', ['request' ,'target_tile_id', 'source_tile_id', 'source_tile_data', 'out_tile_data'])"],"metadata":{"collapsed":false},"outputs":[],"execution_count":1},{"cell_type":"code","source":["IM_W, IM_H = 60000, 60000\nIM_W, IM_H = 6000, 6000\nTILE_W, TILE_H = 2000, 2000\nTILES_PER_IMAGE = int(IM_W / TILE_W * IM_H / TILE_H)\n\ndef stack(arr_list, axis = 0): \n    \"\"\"\n    since numpy 1.8.2 does not have the stack command\n    \"\"\"\n    assert axis == 0, \"Only works for axis 0\"\n    return np.vstack(map(lambda x: np.expand_dims(x,0), arr_list))\n\ndef get_output_tiles(req_str):\n    return [(req_str,tile_id(req_str, -1, x, y,\"out\")) for x in np.arange(0, IM_W, TILE_W)\n            for y in np.arange(0, IM_H, TILE_H)]\n\ndef calc_input_tiles(o_tile, n_size = 1, delay = 0):\n    \"\"\"\n    A simple 3x3 neighborhood with a delay second delay\n    \"\"\"\n    time.sleep(delay)\n    return [(o_tile, o_tile._replace(\n                type = \"in\",\n                x=ix, \n                y=iy)) \n            for ix in np.arange(o_tile.x-n_size*TILE_W,o_tile.x+(n_size+1)*TILE_W, TILE_W) if ix>=0\n            for iy in np.arange(o_tile.y-n_size*TILE_H,o_tile.y+(n_size+1)*TILE_H, TILE_H) if iy>=0]\n\ndef pull_input_tile(i_tile, delay = 2.5):\n    \"\"\"\n    Generates a random, but unique 8 bit input tile given the source tile id i_tile\n    \"\"\"\n    assert i_tile.type.find(\"in\")==0\n    time.sleep(delay)\n    np.random.seed(i_tile.__hash__() % 4294967295) # should make process deterministic\n    return np.random.uniform(-127, 127, size = (TILE_W, TILE_H, 4)).astype(np.int16)\n\nRECON_TIME = 1.0\ndef partial_reconstruction(src_tile, targ_tile, tile_data, delay = RECON_TIME):\n    time.sleep(delay)\n    out_data = tile_data.copy()\n    out_data[out_data>20] = 0\n    return out_data\n\ndef combine_reconstructions(many_slices):\n    \"\"\"\n    Bring a number of partial reconstructions together\n    \"\"\"\n    return np.sum(stack(many_slices,0),0)\n\ndef full_reconstruction(src_tiles, pr_delay = RECON_TIME/2):\n    \"\"\"\n    Run the full reconstruction as one step\n    \"\"\"\n    out_images = [partial_reconstruction(src.source_tile_id, src.target_tile_id, src.source_tile_data, delay = pr_delay) for src in src_tiles]\n    return combine_reconstructions(out_images)\n    "],"metadata":{"collapsed":false},"outputs":[],"execution_count":2},{"cell_type":"code","source":["req_rdd = sc.parallelize(['test1', 'test2'])\nout_tile_rdd = req_rdd.flatMap(get_output_tiles).repartition(100)\nall_tile_rdd = out_tile_rdd.flatMapValues(calc_input_tiles).map(lambda x: tile_item(x[0], x[1][0], x[1][1], None, None))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":3},{"cell_type":"code","source":["print(all_tile_rdd.first(), all_tile_rdd.count())"],"metadata":{"collapsed":false},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["# Simple / Naive Approach\nThe Simple Naive Approach does not attempt to group together operations reading the same input times and just runs each independently"],"metadata":{}},{"cell_type":"code","source":["# parallel reading of the data\nall_tile_rdd_data = all_tile_rdd.map( lambda i: i._replace(source_tile_data = pull_input_tile(i.source_tile_id)))\ndef ti_full_reconstruction(x):\n    k, src_tiles = x\n    return k, full_reconstruction(src_tiles)\n# parallel combining of the tiles\nrecon_tiles_rdd = all_tile_rdd_data.groupBy(lambda x: x.target_tile_id).map(ti_full_reconstruction)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%%time\nall_shapes = recon_tiles_rdd.mapValues(lambda x: x.shape).collect()\nprint('All Results', len(all_shapes))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["# Grouped Read Approach"],"metadata":{}},{"cell_type":"code","source":["def grp_tile_read(x):\n    src_tile_id, tile_items = x\n    tile_data = pull_input_tile(src_tile_id)\n    return [i._replace(source_tile_data = tile_data) for i in tile_items]\nsingle_read_tiles_rdd = all_tile_rdd.groupBy(lambda x: x.source_tile_id).flatMap(grp_tile_read)\ngr_recon_tiles_rdd = single_read_tiles_rdd.groupBy(lambda x: x.target_tile_id).map(ti_full_reconstruction)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%%time\nall_shapes = gr_recon_tiles_rdd.mapValues(lambda x: x.shape).collect()\nprint('All Results', len(all_shapes))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["# Second Approach\nPartial Results to Final Results"],"metadata":{}},{"cell_type":"code","source":["def ti_partial_reconstruction(in_tile_item):\n    return in_tile_item._replace(\n        out_tile_data = partial_reconstruction(in_tile_item.source_tile_id,\n                                               in_tile_item.target_tile_id,\n                                               in_tile_item.source_tile_data),\n        source_tile_data = None # throw out the old data\n        )\n\ndef ti_partial_collect(x):\n    k, in_tile_items = x # don't need the key\n    assert len(in_tile_items)>0, \"Cannot provide empty partial collecton set\"\n    iti_list = list(in_tile_items)\n    all_part_recon = map(lambda x: x.out_tile_data, in_tile_items)\n    \n    return (k, combine_reconstructions(all_part_recon))\n\n# group together all files by source tile and then read that source tile and put in into every item\nsingle_read_tiles_rdd = all_tile_rdd.groupBy(lambda x: x.source_tile_id).flatMap(grp_tile_read)\n# run a partial reconstruction on every item\npartial_recon_tiles_rdd = single_read_tiles_rdd.map(ti_partial_reconstruction)\n# combine all the partial reconstructions to create the final reconstruction\nfull_recon_tiles_rdd = partial_recon_tiles_rdd.groupBy(lambda x: x.target_tile_id).map(ti_partial_collect)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%%time\nall_shapes = full_recon_tiles_rdd.mapValues(lambda x: x.shape).collect()\nprint('All Results',len(all_shapes))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# check every element to ensure they are identical\nfor i, (x,y) in enumerate(zip(full_recon_tiles_rdd.first()[1].flatten(), \n                              recon_tiles_rdd.first()[1].flatten())):\n    assert x == y, \"Index {} doesnt match ({} != {})\".format(i, x, y)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["# Compressing Tiles and Output\nHere we compress the tiles and then write the output"],"metadata":{}},{"cell_type":"code","source":["from io import BytesIO\ndef compress_tile(in_tile):\n    out_stream = BytesIO()\n    np.savez_compressed(out_stream, out_tile = in_tile)\n    out_stream.seek(0) # restart at beginning\n    return \"\\n\".join(out_stream.readlines())\n\n\nall_image_rdd = full_recon_tiles_rdd.mapValues(compress_tile).groupBy(lambda x: x[0].img_id)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%%time\nall_image_rdd.saveAsPickleFile('big_image_out.jp2')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["# Light Keys and Heavy Values\n## DataFrame vs RDD"],"metadata":{}},{"cell_type":"code","source":["# Make a cached performant RDD to start with\nx_to_tile = lambda x: tile_id(\"\",0,int(x),int(x),\"in\")\nkeys_rdd = sc.parallelize(range(101),20).map(x_to_tile).cache()\n_ = keys_rdd.collect()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["# Using Standard RDDs\nUsing a standard RDD the entire data has to be loaded / processed in order to check the x value."],"metadata":{}},{"cell_type":"code","source":["kimg_rdd = keys_rdd.map(lambda x: (x, pull_input_tile(x)))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%%time\nkimg_rdd.filter(lambda kv_data: kv_data[0].x>99).collect()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["# Using DataFrames\nUsing DataFrames the exact same query can be conducted without looking at the image column at all. Here the image column is only examined at the very end."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions as F\nimport pyspark.sql.types as sq_types\nkmeta_df = sqlContext.createDataFrame(keys_rdd.map(lambda x: x._asdict()))\n# applying python functions to DataFrames is more difficult and requires using typed UDFs\ntwod_arr_type = sq_types.ArrayType(sq_types.ArrayType(sq_types.IntegerType()))\n# the pull_input_tile function is wrapped into a udf to it can be applied to create the new image column\n# numpy data is not directly supported and typed arrays must be used instead therefor we run the .tolist command\npull_tile_udf = F.udf(lambda x: pull_input_tile(x_to_tile(x)).tolist(), returnType = twod_arr_type)\nkimg_df = kmeta_df.withColumn('Image', pull_tile_udf(kmeta_df['x']))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%%time\ns_query = kimg_df.where(kimg_df['x']>99)\ns_query.show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%%time\nkimg_df.where(kimg_df['x']==27).show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# show the array to make sure it matches\niv_arr = np.array(s_query.first().Image)\niv_arr"],"metadata":{"collapsed":false},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# test cartesian product later kmeta_df.join(s_query, on = [\"x\", \"y\"], how = 'inner').show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":28},{"cell_type":"code","source":["s_query.explain()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":29},{"cell_type":"code","source":["pt_2d_type = sq_types.StructType(fields = [sq_types.StructField(\"_1\", sq_types.IntegerType()), \n                     sq_types.StructField(\"_2\", sq_types.IntegerType())])\nbrightest_point_udf = F.udf(lambda x: np.unravel_index(np.argmax(x), dims = np.shape(x)), returnType = pt_2d_type)\nmean_point_udf = F.udf(lambda x: float(np.mean(x)), returnType = sq_types.DoubleType())\nkimg_max_df = kimg_df.withColumn('MeanPoint', mean_point_udf(kimg_df['Image']))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":30},{"cell_type":"code","source":["%%time\nfour_img_query = kimg_max_df.where(kimg_max_df['x']>88).where(kimg_max_df['y']<92)\nfour_img_query.show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":31},{"cell_type":"code","source":["four_img_query.explain()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":32},{"cell_type":"code","source":["kimg_max_df.registerTempTable(\"ImageTable\")"],"metadata":{"collapsed":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":["kmeta_df.registerTempTable(\"MetaTable\")"],"metadata":{"collapsed":false},"outputs":[],"execution_count":34},{"cell_type":"code","source":[""],"metadata":{"collapsed":false},"outputs":[],"execution_count":35},{"cell_type":"code","source":[""],"metadata":{"collapsed":true},"outputs":[],"execution_count":36},{"cell_type":"code","source":[""],"metadata":{"collapsed":false},"outputs":[],"execution_count":37},{"cell_type":"code","source":[""],"metadata":{"collapsed":true},"outputs":[],"execution_count":38}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3.0},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"BenchmarkFunctions","notebookId":2047689440275051},"nbformat":4,"nbformat_minor":0}
